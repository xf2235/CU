{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mELOi-lUIMF1"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sG77NaXJIMF-",
    "outputId": "3de66f62-2216-4cfc-c937-183c959c9443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   InMichelin Restaurant Name  Food  Decor  Service  Price\n",
      "0           0  14 Wall Street    19     20       19     50\n",
      "1           0             212    17     17       16     43\n",
      "2           0        26 Seats    23     17       21     35\n",
      "3           1              44    19     23       16     52\n",
      "4           0               A    23     12       19     24\n"
     ]
    }
   ],
   "source": [
    "#Import some example data\n",
    "\n",
    "import pandas as pd\n",
    "# target = InMichelin, whether or not a restaurant is in the Michelin guide\n",
    "data = pd.read_csv(\"http://gattonweb.uky.edu/sheather/book/docs/datasets/MichelinNY.csv\", encoding=\"latin_1\")\n",
    "print(data.head())\n",
    "\n",
    "#update data to set up for train test split\n",
    "data = data.loc[:, data.columns != 'Restaurant Name']\n",
    "y = data['InMichelin']\n",
    "X = data.loc[:, data.columns != 'InMichelin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M387P_MHIMGV",
    "outputId": "1a3abc07-1ac2-4c75-f173-7e8f81543840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.38181614  0.07433425 -0.15691054  0.08189853]]\n",
      "Training set score: 0.797\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Set up training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42) \n",
    "\n",
    "#Note: random_state ensures same data will be generated for example each time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(penalty='none').fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ycvi2emNIMGy",
    "outputId": "1b7ca300-bb18-425f-fee7-c71697ddb627"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg\n",
    "\n",
    "#Use ?LogisticRegression() for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fODA7tsxIMHI"
   },
   "source": [
    "## Logistic Regression in statsmodels package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "igNMntjKIMHN",
    "outputId": "7e055257-f3e2-4646-b8a4-3344afd57db7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>InMichelin</td>    <th>  No. Observations:  </th>  <td>   123</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   118</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -57.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 19 Jun 2022</td> <th>  Deviance:          </th> <td>  114.53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:24:50</td>     <th>  Pearson chi2:      </th>  <td>  254.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>6</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  -10.6490</td> <td>    2.588</td> <td>   -4.115</td> <td> 0.000</td> <td>  -15.722</td> <td>   -5.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Food</th>    <td>    0.3818</td> <td>    0.148</td> <td>    2.572</td> <td> 0.010</td> <td>    0.091</td> <td>    0.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Decor</th>   <td>    0.0743</td> <td>    0.103</td> <td>    0.720</td> <td> 0.471</td> <td>   -0.128</td> <td>    0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Service</th> <td>   -0.1569</td> <td>    0.147</td> <td>   -1.070</td> <td> 0.285</td> <td>   -0.444</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Price</th>   <td>    0.0819</td> <td>    0.036</td> <td>    2.269</td> <td> 0.023</td> <td>    0.011</td> <td>    0.153</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:             InMichelin   No. Observations:                  123\n",
       "Model:                            GLM   Df Residuals:                      118\n",
       "Model Family:                Binomial   Df Model:                            4\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -57.266\n",
       "Date:                Sun, 19 Jun 2022   Deviance:                       114.53\n",
       "Time:                        10:24:50   Pearson chi2:                     254.\n",
       "No. Iterations:                     6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -10.6490      2.588     -4.115      0.000     -15.722      -5.576\n",
       "Food           0.3818      0.148      2.572      0.010       0.091       0.673\n",
       "Decor          0.0743      0.103      0.720      0.471      -0.128       0.277\n",
       "Service       -0.1569      0.147     -1.070      0.285      -0.444       0.131\n",
       "Price          0.0819      0.036      2.269      0.023       0.011       0.153\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_new = sm.add_constant(X_train)\n",
    "\n",
    "model = sm.GLM(y_train, X_train_new, family=sm.families.Binomial()).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_VyFy5-IMHc"
   },
   "source": [
    "## Logistic Regression with constraints on size of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kW-yfnUIMHf",
    "outputId": "40c89905-2503-45c7-9563-015853d7c9e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.38171368  0.07433904 -0.15682846  0.08189077]]\n",
      "Training set score: 0.797\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 1]\n",
      "logreg.predict prob [[0.85254277 0.14745723]\n",
      " [0.84461604 0.15538396]\n",
      " [0.67682771 0.32317229]\n",
      " [0.13745703 0.86254297]\n",
      " [0.86696123 0.13303877]\n",
      " [0.86173258 0.13826742]\n",
      " [0.83064887 0.16935113]\n",
      " [0.89750002 0.10249998]\n",
      " [0.53908214 0.46091786]\n",
      " [0.79532536 0.20467464]]\n"
     ]
    }
   ],
   "source": [
    "# Smaller C will constrain Betas more.  It's a tuning parameter we can find using gridsearch.\n",
    "\n",
    "#C=100, compare coefs to regular model above.\n",
    "logreg = LogisticRegression(C=100, penalty='l2').fit(X_train, y_train) \n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))\n",
    "print(f\"logreg.predict prob {logreg.predict_proba(X_test[0:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RCMm5WCIMHq",
    "outputId": "6ecf5963-8d61-4fbf-8e33-6e03fd1b0b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 0.37187726  0.07490079 -0.14897911  0.08113593]]\n",
      "Training set score: 0.797\n",
      "Test set score: 0.780\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#C=1, compare coefs to above models.\n",
    "logreg = LogisticRegression(C=1, penalty='l2').fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpGBOiXCIMIf",
    "outputId": "77a889f0-8bf1-4545-e126-82069c5deade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[0.00549429 0.00672568 0.00502413 0.02866617]]\n",
      "Training set score: 0.699\n",
      "Test set score: 0.732\n",
      "logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#C=.0001, compare coefs to above models.\n",
    "\n",
    "#Does the model's prediction power get better or worse??\n",
    "\n",
    "logreg = LogisticRegression(C=.0001, penalty='l2').fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJlOnuHVnZJd",
    "outputId": "5a24f8ec-a540-4e2f-d4f8-81c8f3ca004a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[-0.02290056  0.          0.          0.00967782]]\n",
      "Training set score: 0.699\n",
      "Test set score: 0.732\n",
      "logreg.predict: [0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#What if we want to use an l1 penalty instead?  Change penalty to 'l1' and solver to 'liblinear'.\n",
    "\n",
    "#Does the model's prediction power get better or worse??\n",
    "\n",
    "logreg = LogisticRegression(C=.01, penalty='l1',solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT3rYS9jIMIn"
   },
   "source": [
    "## Multiclass models (Multinomial model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1JVt7blIMIp",
    "outputId": "834a6922-0092-4f1d-ac32-13c96358196c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "iris\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(iris.feature_names )# X variable names\n",
    "print(X[0:5]) # first five rows of data\n",
    "\n",
    "print(iris.target_names) #target categories\n",
    "print(np.unique(y)) #target values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KuMM3GkSIMIv"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",max_iter=10000,).fit(X,y) #Note the argument changes to LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAjgz06fIMI3",
    "outputId": "1568c278-e1ba-41c3-b170-459ad95a7981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(logreg.predict(X)) #uses softmax function to predict new X data, but I am being lazy and using X data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Logistic Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
