{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxt4c9351wcK"
   },
   "source": [
    "# Standard Scaler \n",
    "(centering mean on zero and std dev on 1 for each variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1654282569894,
     "user": {
      "displayName": "Michael D. Parrott",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "1M3_CKp91wcO",
    "outputId": "a5945b38-a55b-49e3-8712-71ce26d084f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The function scale provides a quick and easy way to perform this operation on a single array-like dataset:\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "#Build dataset with three columns and three rows\n",
    "#Structure is visually the same as a typical dataframe (i.e.-columns are up and down and rows side to side)\n",
    "\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75Yzq0oO1wcb"
   },
   "source": [
    "To calculate the standard scaler for each column subtract column mean from observation value and divide result by the column standard deviation.\n",
    "Scaled data has zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1654282612000,
     "user": {
      "displayName": "Michael D. Parrott",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "Mb3oK6_z1wcd",
    "outputId": "22c8f0f7-ee57-476b-e976-428faa38127f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled.mean(axis=0)) # axis=0 refers to columns\n",
    "print(X_scaled.std(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShA654gK1wcm"
   },
   "source": [
    "# Using StandardScaler()\n",
    "The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1654282791743,
     "user": {
      "displayName": "Michael D. Parrott",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "bGGKqNw61wcp",
    "outputId": "0146d8ed-ccdb-44db-b58f-6d63e64219d9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we set up the the standard scaler to the X_train data using fit()\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "print(scaler) # show details of scaler object\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Here we apply the fit standard scaler to the X_train data using transform()\n",
    "\n",
    "scaler.transform(X_train)                         \n",
    "\n",
    "# print(scaler.mean_) #print the means  per column                           \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1654282804185,
     "user": {
      "displayName": "Michael D. Parrott",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "JbBLEk6z1wcw",
    "outputId": "6d78ff06-176d-4253-d591-eeacfc4d3a42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.44948974,  1.22474487, -0.26726124]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The scaler instance can then be used on new data to transform it the same way it did on the training set:\n",
    "#Note that we are scaling new data to the scale built from the training data.  \n",
    "\n",
    "X_test = [[-1., 1., 0.]]\n",
    "scaler.transform(X_test) # Transform x_test before running a model, for example        \n",
    "\n",
    "#It is also possible to disable either centering or scaling by either passing with_mean=False or with_std=False\n",
    "#to the constructor of StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MgdjWUfO1wc4",
    "outputId": "fdca3ad7-53bd-46a0-f90c-b15700688489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258270793642969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use transformed data with a model:\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridge.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9Ue8Llc1wdA"
   },
   "source": [
    "# Using MinMaxScaler\n",
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JDTY8pVR1wdC",
    "outputId": "56b1bc33-c9c5-4118-c872-5edee1baa92b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is an example to scale an example data matrix to the [0, 1] range:\n",
    "\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train) # fit_transform does both at once.  It's a little faster.\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E6aOxAhf1wdK",
    "outputId": "2782a648-03b7-4133-bb96-d6a6c6d35271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And again we can then use the scaler to transform new data.\n",
    "#Note once more that we are scaling new data to the scale built from the training data.  \n",
    "\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DarDDtLH1wdS"
   },
   "source": [
    "The formula for a min-max transformation is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DCKXdBr1wdU"
   },
   "source": [
    "For each value in a column of X, subtract the minimium value of the column and divide result by the the result of the maximum value minus the minimum value of the column\n",
    "(X observation value - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1570110153440,
     "user": {
      "displayName": "Michael D. Parrott",
      "photoUrl": "",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "hl9fSgmu1wdW",
    "outputId": "5d7d45c4-5d98-425b-bcae-8efed4aafc56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6499508833957255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use transformed data with a model:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridge.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5J3y6tR1wde"
   },
   "source": [
    "# Using scikit-learn Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1570110158470,
     "user": {
      "displayName": "Michael D. Parrott",
      "photoUrl": "",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "Xb4S0D311wdg",
    "outputId": "48757e1a-dfc6-47fe-f8b2-287f92ca453c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634588456488905"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is the code for a ridge model with the standard transformation...\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler #now we don't need to add preprocessing. before calls to StandardScaler()\n",
    "\n",
    "X, y = boston.data, boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridge.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1570110161107,
     "user": {
      "displayName": "Michael D. Parrott",
      "photoUrl": "",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "4oRC7Lqx1wdo",
    "outputId": "3a8046bd-16f7-4416-fb6b-66ccd5fbd81c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634588456488905"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the much cleaner pipeline version:\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnJ662Hr1wdw"
   },
   "source": [
    "# Pipeline and GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e1M6S05A1wdy"
   },
   "outputs": [],
   "source": [
    "# We need to pay attention to names of pipeline steps when we use GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1570110163790,
     "user": {
      "displayName": "Michael D. Parrott",
      "photoUrl": "",
      "userId": "13070067452828357902"
     },
     "user_tz": 240
    },
    "id": "a2WYDy1R1wd5",
    "outputId": "168e7262-36cf-42f9-deeb-e59f0289095b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardscaler', StandardScaler()), ('kneighborsregressor', KNeighborsRegressor())]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "print(knn_pipe.steps) # names in single quotes (i.e.-'standardscaler' and 'kneighborsregressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9b9tmtm-1weA",
    "outputId": "e2eb3d58-7bf2-40eb-d108-84dcc80624c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsregressor__n_neighbors': 7}\n",
      "0.5999825126971097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "\n",
    "#refer to step name with two underscores before argument name when...\n",
    "#you build a parameter grid\n",
    "\n",
    "param_grid = {'kneighborsregressor__n_neighbors': range(1, 10)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Preprocessing Data (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
