{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0d4710-b24e-409c-92b5-1d7f0f9c98c6",
   "metadata": {},
   "source": [
    "# Mid-term Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa16948-e518-4b40-89ff-ae2e389d3d52",
   "metadata": {},
   "source": [
    "## 1.  Import the spam dataset and print the first six rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb70ab0-95f3-46d9-a4a9-c889ab92e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.linear_model import LogisticRegression as GLM\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "928aa19a-46f0-419f-8cc2-7f83be24bdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv('C:/Users/Administrator/Desktop/CU/CU_SM2022/QMSSS5073_001_2022_2 - MACHINE LEARNING SOC SCI/mt/spam_dataset.csv')\n",
    "spam.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff8270-0f7f-4851-9a16-3a4ad8e625dd",
   "metadata": {},
   "source": [
    "## 2.  Read through the documentation of the original dataset here:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "\n",
    "The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbc325-1828-4a3c-8d9e-38407175a410",
   "metadata": {},
   "source": [
    "- I think \"word_freq_000:\", \"char_freq_\\\\$:\", and \"capital_run_length_longest:\" will be important predictors in the model of spam. First, most spam are delusive because it attract people with fraud with receiving a bonus, a check in a greatcertain amount of number, for example, click to get \\\\$1,000! Second, spam usually come with money(\\\\$) as bait. Typically, there are only a few dollar sign in common contact. Most contract with numerous dollar signs are in pdf or other file format rather than text. Third, spam tend to use longer capitalized exaggerated words to attract people's customer, for example, \"INCREDIBLE COUPONS FOR \\\\$1,000!\" which seldom appears in normal email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a8853-a630-4462-9b17-b2b63a6b6e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.  Visualize the univariate distribution of each of the variables in the previous question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b642c2a-b0ff-4cf2-9c0e-ed504a828cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(spam['word_freq_000:'])\n",
    "plt.xlabel('percentage of words in the e-mail that match 000')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f9e020d-edcb-42da-bad6-1f36639aa76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(spam['char_freq_$:'])\n",
    "plt.xlabel('percentage of characters in the e-mail that match $')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8729f0c6-266e-4283-8d03-b28521c82488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(spam['capital_run_length_longest:'])\n",
    "plt.xlabel('length of longest uninterrupted sequence of capital letters')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e38953-3cdb-498d-af5b-8af7ec735a14",
   "metadata": {},
   "source": [
    "## 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675afbfe-4fda-419c-bba8-94738049e097",
   "metadata": {},
   "source": [
    "- 1. K Nearest Neighbors Classifier (KNeighborsClassifier) \n",
    "- 2. Logistic Regression(LogisticRegression) \n",
    "- 3. Decision Tree Classifier (DecisionTreeClassifier) \n",
    "- 4. Bagging Classifier (BaggingClassifier) \n",
    "- 5. Random Forest Classifier (RandomForestClassifier) \n",
    "- 6. Gradient Boost (GradientBoostingClassifier) \n",
    "- 7. XGBoosting Classifier (XGBClassifier) \n",
    "- 8. Support Vector Machine (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8de45f-b1e6-4ea7-bcf6-231517afdcdd",
   "metadata": {},
   "source": [
    "## 5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d08eb-d8ad-48fc-be04-dd65913b7724",
   "metadata": {},
   "source": [
    "- We ought to seperate data into train and test dataset since we need to prevent our model from overfitting which is more likely appearing in using all data to training the model. If all data are used to train the model, the model might works well in the sample data, but high likely won't have a good prediction in data unseen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2657ee-299b-45b2-a3a1-94eaee2c59ca",
   "metadata": {},
   "source": [
    "## 6. What is k-fold cross validation and what do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729df47b-e77e-48d3-892f-d8fa3e1746c3",
   "metadata": {},
   "source": [
    "- K-fold cross validation divides the sample data into k groups (folds). We use it for evaluating the machine learning model since certain models might perform well or terrible by chance with certain train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffe33c-b305-4bca-a71c-759ac3e76a68",
   "metadata": {},
   "source": [
    "## 7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbab83-95b7-4a6c-b5f2-ad97b150c932",
   "metadata": {},
   "source": [
    "- K-fold cross validation is different from stratified k-fold cross validation. K-fold cross validation divides the sample dataset into k folds randomly. Stratified k-fold cross validation divides sample dataset into k folds with same proportion of observations with a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3a880-a980-44f2-b3f3-c30adadf54d7",
   "metadata": {},
   "source": [
    "## 8. Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e11af-b9f4-48f8-9367-9f465ff313f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A) On test data directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4846ca79-bdb8-4c18-b773-dcbecb582171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train & test\n",
    "X = spam[[\"word_freq_000:\", \"char_freq_$:\", \"capital_run_length_longest:\"]]\n",
    "y = spam.spam\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c75faf-238e-4eba-8f5a-45370ef1907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knc score: 0.8123370981754996\n"
     ]
    }
   ],
   "source": [
    "# KNN without cv\n",
    "knc = KNC(n_neighbors=5).fit(X_train, y_train)\n",
    "knc_score = knc.score(X_test, y_test)\n",
    "print(f\"knc score: {knc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b73b78e6-68c3-4646-abb4-8a07efdd4514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'n_neighbors': 6}\n",
      "Best score: 0.8020289855072462\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': range(1, 10)}\n",
    "\n",
    "grid = GridSearchCV(KNC(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameter: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f23fb6-e6ab-4d27-bfa4-ec4139142bb7",
   "metadata": {},
   "source": [
    "- The reason why I choose 5 as number of neighbors are that \n",
    "1) Number of neighbors should be an ODD number in case there is a tie. \n",
    "2) Number of neighbors cannot be too small, for example 1, because the noise will have a higher influence on the result.\n",
    "3) Number of neighbors cannot be too large, for example over 10, since it is computationally expensive.\n",
    "4) k=sqrt(n) is one way, but it does not work well in this case.\n",
    "5) The sample size is not big and hyper prarmeter tuning shows that the best number of neighbors is 6.\n",
    "6) I choose closest two to 6, which are 5 and 7. 5 performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b2502-3d9c-4f60-aac3-73bd328d1d4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B) using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8482959d-899f-4c80-b470-9206348a0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cv\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3c61851-6214-4da5-a330-1d8aa7e0589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold knc score: 0.776755128929042\n"
     ]
    }
   ],
   "source": [
    "# KNN with cv\n",
    "knc = KNC(n_neighbors=5)\n",
    "knc_kfold = mean(cross_val_score(knc, X_test, y_test, cv=kfold))\n",
    "print(f\"KFold knc score: {knc_kfold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46868061-fc71-4f0a-b701-8e0e7349a00a",
   "metadata": {},
   "source": [
    "- knc score: 0.8123370981754996\n",
    "- KFold knc score: 0.7965217391304348\n",
    "\n",
    "It is not surprising that score in KNN without cross validation is higher than KNN with KFold cross validation since K-fold cross validation divides the sample dataset into k folds randomly to evaluate the general performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b8994-f36b-44a3-8d88-fa2ebdd8d768",
   "metadata": {},
   "source": [
    "## 9. Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692ea79-18c8-477f-9a3e-1edb22942368",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A) On test data directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ace5412-5b64-45f6-8ad8-7dbd7f12ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm score: 0.8071242397914856\n"
     ]
    }
   ],
   "source": [
    "# logistic regression without cv\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "glm = GLM(C=0, penalty='none').fit(X_train, y_train)\n",
    "glm_score = glm.score(X_test, y_test)\n",
    "print(f\"glm score: {glm_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a31d88-aa3f-4c91-9d66-4a4f4c95b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm lasso score: 0.8062554300608167\n"
     ]
    }
   ],
   "source": [
    "glm_lasso = GLM(penalty='l1', solver='liblinear').fit(X_train, y_train)\n",
    "print(f\"glm lasso score: {glm_lasso.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "637c9573-7f91-408e-b9f4-e7d712b8f536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'C': 0.0, 'penalty': 'none'}\n",
      "Best score: 0.8150724637681159\n"
     ]
    }
   ],
   "source": [
    "c_space = np.arange(0, 10, 0.5)\n",
    "param_grid = {'C': c_space, 'penalty': ['none', 'l2']}\n",
    "\n",
    "grid = GridSearchCV(GLM(), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameter: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e566d0f-0f75-4882-80fd-6b2c80d5dc40",
   "metadata": {},
   "source": [
    "- The reason why I choose 0 as C(inverse of regularization strength) and none as penalty are\n",
    "1) The sample size is not big and hyper prarmeter tuning shows that the best C is 0 and the best penalty is none.\n",
    "2) Logistic Regression with none penalty perform better than with lasso penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1abb69-b995-4fba-b3de-f1b7f47ed086",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B) using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25a2414-1795-4e5d-97f2-88e98ccd1c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold logistic regression score: 0.8019235836627141\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with cv\n",
    "glm = GLM(C=0, penalty='none').fit(X_train, y_train)\n",
    "glm_kfold = mean(cross_val_score(glm, X_test, y_test))\n",
    "print(f\"KFold logistic regression score: {glm_kfold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99182e25-7b1c-49ec-9f58-2fde49af1a53",
   "metadata": {},
   "source": [
    "- glm score: 0.8071242397914856\n",
    "- KFold logistic regression score: 0.8019235836627141\n",
    "\n",
    "In this case, logistic regression without cross validation perform better than with KFold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd327679-6cbd-480b-a4d7-5a99f672546d",
   "metadata": {},
   "source": [
    "## 10. Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb0ac6-d084-4d4c-8ab0-f10e3e307954",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A) On test data directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30133359-bc30-498f-9b78-87e90f86b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest score: 0.8314509122502172\n"
     ]
    }
   ],
   "source": [
    "# random forest without cv\n",
    "forest = RFC(n_estimators=110, max_depth=9, random_state=5).fit(X_train, y_train)\n",
    "forest_score = forest.score(X_test, y_test)\n",
    "print(f\"forest score: {forest_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "441d06a9-4256-4c20-8bfa-cb1407d66163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'max_depth': 9, 'n_estimators': 110}\n",
      "Best score: 0.8443478260869565\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': np.arange(0,200,10),\n",
    "              'max_depth': range(1, 10)}\n",
    "\n",
    "grid = GridSearchCV(RFC(), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "print(f\"Best parameter: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fc91f-aabb-4fe4-905a-052d253cbec2",
   "metadata": {},
   "source": [
    "- The reason why I choose 110 as number of estimators and 9 as maximum depth\n",
    "1) The sample size is not big and hyper prarmeter tuning shows that the best n_estimators is 110 and the best max_depth is 9.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c93e9f-32bf-4837-99ca-deb553273476",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B) using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b333b84-851f-4edc-943d-86ddba04309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold forest score: 0.8192885375494071\n"
     ]
    }
   ],
   "source": [
    "# random forest with cv\n",
    "forest = RFC(n_estimators=110, max_depth=9, random_state=5).fit(X_train, y_train)\n",
    "forest_kfold = mean(cross_val_score(forest, X_test, y_test))\n",
    "print(f\"KFold forest score: {forest_kfold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cef3b0-a0b7-49bf-9f85-717fb85eb491",
   "metadata": {},
   "source": [
    "- forest score: 0.8314509122502172\n",
    "- KFold forest score: 0.8175494071146245\n",
    "\n",
    "In this case, random forest without cross validation perform better than with KFold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f1d17-0983-489e-a7bc-145b3778899f",
   "metadata": {},
   "source": [
    "## 11. Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be3b0c-eba8-4050-9570-140c5b7da949",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A) On test data directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c35dc6e2-6fc8-4488-ac7a-d3b9316b36db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc score: 0.8062554300608167\n"
     ]
    }
   ],
   "source": [
    "# svc without cv\n",
    "svc = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "svc_score = svc.score(X_test, y_test)\n",
    "print(f\"svc score: {svc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb8ccb-7851-4073-bf65-6b1fbe0f17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_space = np.arange(0.5, 10, 0.5)\n",
    "param_grid = {'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], 'C': c_space}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameter: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6551d-5a79-4714-9a19-d4e0a2b3a814",
   "metadata": {},
   "source": [
    "- The reason why I choose linear as kernel, 1 as C (regularization parameter)\n",
    "1) The sample size is not big and hyper prarmeter tuning shows that best kernel is linear and best C is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5bcf44-31bf-499f-af57-de68e466ba82",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B) using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d99de8-4be4-4d54-b001-62f03e9d7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc with cv\n",
    "svc = SVC(kernel='linear', C=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc2f87bb-f800-45f1-837a-c7c34e5a53d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold svc score: 0.8019160549595332\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "svc_kfold = mean(cross_val_score(svc, X_test, y_test))\n",
    "print(f\"KFold svc score: {svc_kfold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea732e-0556-42b4-8b82-d048fc94e746",
   "metadata": {},
   "source": [
    "- svc score: 0.8062554300608167\n",
    "- KFold svc score: 0.8019160549595332\n",
    "\n",
    "In this case, support vector machine without cross validation perform better than with KFold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391f295-308f-4aea-bd33-65ea9655dfbf",
   "metadata": {},
   "source": [
    "## 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3afe99a-b2de-4f93-bb69-ca1a32bf7972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ml model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knc_score</th>\n",
       "      <td>0.812337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glm_score</th>\n",
       "      <td>0.807124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_score</th>\n",
       "      <td>0.831451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svc_score</th>\n",
       "      <td>0.806255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ml model\n",
       "knc_score     0.812337\n",
       "glm_score     0.807124\n",
       "forest_score  0.831451\n",
       "svc_score     0.806255"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = {'ml model': [knc_score, glm_score, forest_score, svc_score]}\n",
    "com = pd.DataFrame(comparison, index=['knc_score', 'glm_score', 'forest_score', 'svc_score'])\n",
    "com              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04d1d165-2bc0-4d7e-ac85-586960d3025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spam[['word_freq_remove:', 'word_freq_free:', 'word_freq_credit:']]\n",
    "X_six = pd.concat([X, df], axis=\"columns\")\n",
    "y = spam.spam\n",
    "X_train_six, X_test_six, y_train_six, y_test_six = train_test_split(X_six, y, train_size=0.75, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "311d4511-7626-4730-ac20-f9ef3d426a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest score: 0.8314509122502172\n",
      "forest score with six var: 0.89748045178106\n"
     ]
    }
   ],
   "source": [
    "# random forest with six var\n",
    "forest_six = RFC(n_estimators=40, max_depth=9, random_state=4).fit(X_train_six, y_train_six)\n",
    "forest_score_six = forest_six.score(X_test_six, y_test_six)\n",
    "print(f\"forest score: {forest_score}\")\n",
    "print(f\"forest score with six var: {forest_score_six}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402389e-c306-4ee7-b792-786a845bee7b",
   "metadata": {},
   "source": [
    "- This model with 6 variables predict test data better than your previous models with three variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92da4b-9ba6-4d8d-bc56-560247179143",
   "metadata": {},
   "source": [
    "## 13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22fa6886-2ad0-479a-b8bb-d7ed089cd8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knc score: 0.8123370981754996\n",
      "knc score with six var: 0.8470894874022589\n"
     ]
    }
   ],
   "source": [
    "# KNN with six var\n",
    "knc_six = KNC(n_neighbors=5).fit(X_train_six, y_train_six)\n",
    "knc_score_six = knc_six.score(X_test_six, y_test_six)\n",
    "print(f\"knc score: {knc_score}\")\n",
    "print(f\"knc score with six var: {knc_score_six}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c40934f-1959-4d82-90a6-cb127121d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm score: 0.8071242397914856\n",
      "glm score with six var: 0.8479582971329279\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with six var\n",
    "glm_six = GLM(C=0, penalty='none').fit(X_train_six, y_train_six)\n",
    "glm_score_six = glm_six.score(X_test_six, y_test_six)\n",
    "print(f\"glm score: {glm_score}\")\n",
    "print(f\"glm score with six var: {glm_score_six}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5fb4365-9875-4b11-b62f-4faf462bea22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc score: 0.8062554300608167\n",
      "svc score with six var: 0.8514335360556038\n"
     ]
    }
   ],
   "source": [
    "# svc with six var\n",
    "svc_six = SVC(kernel='linear', C=1).fit(X_train_six, y_train_six)\n",
    "svc_score_six = svc_six.score(X_test_six, y_test_six)\n",
    "print(f\"svc score: {svc_score}\")\n",
    "print(f\"svc score with six var: {svc_score_six}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8470fe2-4286-40c7-b710-99f0fd7d99cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ml model</th>\n",
       "      <th>ml model with six var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knc_score</th>\n",
       "      <td>0.812337</td>\n",
       "      <td>0.847089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glm_score</th>\n",
       "      <td>0.807124</td>\n",
       "      <td>0.847958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_score</th>\n",
       "      <td>0.831451</td>\n",
       "      <td>0.897480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svc_score</th>\n",
       "      <td>0.806255</td>\n",
       "      <td>0.851434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ml model  ml model with six var\n",
       "knc_score     0.812337               0.847089\n",
       "glm_score     0.807124               0.847958\n",
       "forest_score  0.831451               0.897480\n",
       "svc_score     0.806255               0.851434"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = {'ml model': [knc_score, glm_score, forest_score, svc_score],\n",
    "             'ml model with six var': [knc_score_six, glm_score_six, forest_score_six, svc_score_six]}\n",
    "com = pd.DataFrame(comparison, index=['knc_score', 'glm_score', 'forest_score', 'svc_score'])\n",
    "com              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e06df7-b1c5-4a9c-a575-737961bc414e",
   "metadata": {},
   "source": [
    "- I would choose random forest model. Because it has highest prediction accuracy among all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b1650-82a3-47d2-842a-2ae6aa36cedd",
   "metadata": {},
   "source": [
    "## 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc9e5a-4839-420c-98f1-91c1cdff3439",
   "metadata": {},
   "source": [
    "- I would like to add word_freq_urgent (word frequency of urgent) since it easily catch people's eyes with sales effect. There are enormous ways to communicate urgency without saying \"urgent\". Alternatives like \"as soon as possible\" or \"deadline\" are far less likely to be spam.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a435f0-4541-4b98-acc9-237a30594814",
   "metadata": {},
   "source": [
    "## 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dace5f-3671-4b28-b3e9-69043b561ec4",
   "metadata": {},
   "source": [
    "For continuous dependent variable, there are following machine learning models\n",
    "- 1. K Nearest Neighbors Regreesion (KNeighborsRegressor) \n",
    "- 2. Linear Regression (LinearRegression)\n",
    "- 3. Decision Tree Regression (DecisionTreeRegressor)\n",
    "- 4. Bagging Regression (BaggingRegressor)\n",
    "- 5. Random Forest Regression (RandomForestRegressor)\n",
    "- 6. Gradient Boost Regression (GradientBoostingRegressor)\n",
    "- 7. XGBoosting Regression (XGBRegressor) \n",
    "- 8. Support Vector Machine (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f036b453-9742-4040-9bdc-08c5eedb06d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8114682884448305\n"
     ]
    }
   ],
   "source": [
    "# Goal is to predict ytest for each model and then use PREDICTIONS FROM EACH MODEL to select final predictions\n",
    "\n",
    "# Need to set up a standard for selecting final prediction:\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Estimators arg is giving each estimator a name for references in functions like GridsearchCV\n",
    "\n",
    "# voting='hard' takes majority vote of each predicted value to select final prediction for ytest\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('knc', knc_six), ('glm', glm_six), ('rfc', forest_six), ('svc', svc_six)],\n",
    "    voting='hard') \n",
    "vmodel = voting_clf.fit(X_train, y_train)\n",
    "print(vmodel.score(X_test, y_test)) #return accuracy of voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e764005-c756-4866-92dd-99bde290e748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc score: 0.8062554300608167\n",
      "svc score with six var: 0.8514335360556038\n",
      "0.8288444830582102\n"
     ]
    }
   ],
   "source": [
    "# 'soft' voting takes the predicted probabilities of each model and choose the highest value#\n",
    "svc_six = SVC(kernel='linear', C=1, probability=True).fit(X_train_six, y_train_six)\n",
    "svc_score_six = svc_six.score(X_test_six, y_test_six)\n",
    "print(f\"svc score: {svc_score}\")\n",
    "print(f\"svc score with six var: {svc_score_six}\")\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('knc', knc_six), ('glm', glm_six), ('rfc', forest_six), ('svc', svc_six)],\n",
    "    voting='soft') \n",
    "vmodel = voting_clf.fit(X_train, y_train)\n",
    "print(vmodel.score(X_test, y_test)) #return accuracy of voting classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
