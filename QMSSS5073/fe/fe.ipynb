{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78562a3-b298-4630-996c-8418e0d4b177",
   "metadata": {},
   "source": [
    "# Final Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795885a-b172-4d56-b80b-1cd17c66b1c8",
   "metadata": {},
   "source": [
    "## 1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e02b7-22af-4d29-80e1-f5e1bb75f027",
   "metadata": {},
   "source": [
    "Regression models, including linear regression, ridge , lasso etc., are useful for ruling out alternative explanations through control variables and that allow us to observe substantively meaningful information from model coefficients. We can standardize the variables and monitor the coefficients to not only take off other reasons but also find out the relative important factors. Additionally, shrinkage methods, ridge and lasso, are helpful to select meaningful variables from all features by reducing coefficients of variables.\n",
    "On the other hand, classification, such as decision tree, bagging, random forest, XGBoost etc. accurately assign test data into specific categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41a403-e86c-4f70-83c9-ee7f1ec25244",
   "metadata": {},
   "source": [
    "## 2. Describe the main differences between supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee7640-8340-4f31-ab39-2c44be0a46d6",
   "metadata": {},
   "source": [
    "Supervised and unsupervised learning are belonged to machine learning. Howeverm there are several main differences between them. Firsly, supervised learning uses labeled datasets while unsupervised learning leverage unlabeled datasets. Secondly, the goal of supervised learning is to predict outputs for new data while the goal of unsupervised learning is to get insights from large volumes of new data. Thirdly, pricing prediction, sentiment analysis, spam detection etc. can be found by supervised learning while market segements, anomaly detection, recommendation engines etc. can be used in unsupervised machine learning. Fourthly, unsupervised learning is more computationally complex than supervised learning since it does not has corresponding ouputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27676d9b-f8d1-431b-91bc-c4efd52466fd",
   "metadata": {},
   "source": [
    "## 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820057b-0996-4ae6-a0fb-1ac847f064cc",
   "metadata": {},
   "source": [
    "Supervised learning is the primary approach that is used by machine learning practitioners because of measurability, actionability, meaningfulness, and achievability. Machine learning practitioners can dig further into the predictions based on labeled datasets given by expertise. Models with high cross-validation and test scores are more likely to predict in higher accuracy given new datasets. On the other hand, unsupervised learning analyzes and clusters unlabeled datasets. It discovers hidden patterns in data with much less human intervention relative to supervised. In this case, practitioners are hard to measure and identify without output since there is no specific benchmark. Unsupervised learning is used for three main tasks-clustering, association and dimensionality reduction. Specifically, unsupervised learning models clusters different groups, associates variables' relationship, and reduces features' dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdba4b-5dc6-46da-9274-124cf66ec654",
   "metadata": {},
   "source": [
    "## 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2aa19-c3a5-4a72-8368-b008a0fd6510",
   "metadata": {},
   "source": [
    "For unsupervised learning modeling, there are following approaches we covered\n",
    "- Clustering\n",
    "     - Hierarchical Clustering\n",
    "     - Kmeans Clustering\n",
    "- Association\n",
    "- Dimensionality Reduction\n",
    "     - PCA\n",
    "\n",
    "Unsupervised learning models are utilized for three main tasksâ€”clustering, association, and dimension reduction. \n",
    "- For clustering, We have covered hierarchical clustering and k-means clustering. Hierarchical clustering groups observations based on the euclidean distance without having a fixed number of clusters. On the other hand, k-means clustering groups observations minimizing within-cluster variance a pre-specified k clusters. Therefore, if there are more domain knowledge and expertise experience to determine number of cluster, k-means is better.\n",
    "- For association, even though we did not cover in this semester, it is a very important section in unsupervised learning. I have go through it from online resources, there are two basic recommendation systems, user-based and item-based, in association.\n",
    "- For dimension reduction, Principle Component Analysis (PCA) reduces the number of features in the dataset and maintains the principal features. Nonetheless, PCA is less effective with a non-linear datasets since PCA generates several linear hyperplanes. In this case, manifold learning is helpful for fitting non-linearly structured data by rotating, scaling or re-orienting the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb27429-60c3-46e7-806a-713646243eda",
   "metadata": {},
   "source": [
    "## 5.  What are the main benefits of using Principal Components Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bde4a2-7d76-4af4-9123-f7e900021245",
   "metadata": {},
   "source": [
    "Principal Components Analysis (PCA) is manily used for dimension reduction. It reduces the dimensionality of large datasets by cutting variables  but still contains most of the important information in the dataset. PCA can lower the computing complexity and storage energy. At the same time, it is visualization friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9219c-5861-491b-90be-06bd57b5e749",
   "metadata": {},
   "source": [
    "## 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c41a1c-4c23-4740-8a62-415ed9c0f62e",
   "metadata": {},
   "source": [
    "Three major differences between a deep multilayer perceptron (MLPs) network and a convolutional neural network (CNNs) model are connection sparsity, convolution and pooling layers, parameter sharing\n",
    "- Connection Sparsity\n",
    "    - MLPs have fully connected layers while CNNs is connection sparsity. In a fully connected layer, each node is connected with each other in the previous layer. On the other hand, CNNs have a filter to move around the input in local region. In this case, CNNs avoids overfitting.\n",
    "- Convolution and Polling Layer\n",
    "    - MLPs only implements dense layer while CNNs use convolution and pooling layers. With the combination of convolution and pooling, CNNs gives location invariant feature detection. In this case, CNNs offer more flexibility.\n",
    "- Parameter Sharing \n",
    "    - MLPs takes vector as inputs while CNNs takes tensor as input. In this case, CNN can handle variety of inputs. At the same time, it saves more storage power and computing complexity. Furthermoe, it understands spatial relation between nearby pixels of image better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914923c-f419-4e81-9f5c-ddfa8ce6871a",
   "metadata": {},
   "source": [
    "## 7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd3f836-beea-474c-8506-93ae791f8ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 50)                1650      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               5100      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 150)               15150     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 755       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,655\n",
      "Trainable params: 22,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential([\n",
    "        Dense(50, activation = 'relu', input_dim=32),\n",
    "        Dense(100, activation = 'relu'),\n",
    "        Dense(150, activation = 'relu'),\n",
    "        Dense(5, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859688e6-bbd9-4662-b86d-c3fb30eeb1f4",
   "metadata": {},
   "source": [
    "## 8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3836fa80-98a6-46df-b91f-7196778e27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 75)                2475      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 150)               11400     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,026\n",
      "Trainable params: 14,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Dense(75, activation = 'relu', input_dim=32),\n",
    "        Dense(150, activation = 'relu'),\n",
    "        Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b6389-8fec-405e-8a27-20e2a16863ba",
   "metadata": {},
   "source": [
    "## 9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e22d8d36-57ba-4b29-9228-076df3605856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 255, 255, 16)      208       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 127, 127, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 126, 126, 28)      1820      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 63, 63, 28)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 111132)            0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                1111330   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,113,358\n",
      "Trainable params: 1,113,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(16, kernel_size=(2, 2), activation='relu', input_shape=(256, 256, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(28, kernel_size=(2, 2), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bae07e-43cf-464e-b39d-8aed4ea419e8",
   "metadata": {},
   "source": [
    "## 10.  Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9422c1-3de6-459d-870b-779ab6ef6abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 255, 255, 32)      416       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 126, 126, 32)      4128      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 63, 63, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 127008)            0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16257152  \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,278,982\n",
      "Trainable params: 16,278,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(256, 256, 3)),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Conv2D(32, kernel_size=(2, 2), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f1074-79bc-47d0-8adf-a6a2812b0203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
